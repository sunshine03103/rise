
hbase MapReduce程序样例入门 
http://www.cnblogs.com/end/archive/2012/12/12/2814819.html

------------------------------------------------------------------------------------------------------------------------

1. 标准的hbase作为数据读取源和输出源的样例

1.1 创建配置信息和作业对象，设置作业的类

Configuration conf = HBaseConfiguration.create();
Job job = new Job(conf, "job name ");
job.setJarByClass(test.class);
Scan scan = new Scan();
TableMapReduceUtil.initTableMapperJob(inputTable, scan, mapper.class, Writable.class, Writable.class, job);
TableMapReduceUtil.initTableReducerJob(outputTable, reducer.class, job);
job.waitForCompletion(true);

1.2 mapper类：
public class mapper extends	TableMapper<KEYOUT, VALUEOUT> { 
	public void map(Writable key, Writable value, Context context) throws IOException, InterruptedException {
		    //mapper逻辑
			context.write(key, value);
		} 
	}
}


1.3 reducer类：
public class countUniteRedcuer extends TableReducer<KEYIN, VALUEIN, KEYOUT> {
	public void reduce(Text key, Iterable<VALUEIN> values, Context context)	throws IOException, InterruptedException {
        //reducer逻辑
		context.write(null, put or delete);
	}
}

------------------------------------------------------------------------------------------------------------------------

2. 标准的hbase作为数据读取源和输出源的样例

2.1 创建配置信息和作业对象，设置作业的类
Configuration conf = HBaseConfiguration.create();

Job job = new Job(conf, "job name ");
job.setJarByClass(test.class); 
job.setMapperClass(mapper.class);
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(LongWritable.class);

FileInputFormat.setInputPaths(job, path); 
TableMapReduceUtil.initTableReducerJob(tableName, reducer.class, job);


2.2 mapper类：              
public class mapper extends Mapper<LongWritable,Writable,Writable,Writable> {
	public void map(LongWritable key, Text line, Context context) {
		 //mapper逻辑
		 context.write(k, one);
	}
}

2.3 reducer类：
public class redcuer extends TableReducer<KEYIN, VALUEIN, KEYOUT> {
	public void reduce(Writable key, Iterable<Writable> values, Context context) throws IOException, InterruptedException {
		//reducer逻辑
		context.write(null, put or delete);
	}
}

------------------------------------------------------------------------------------------------------------------------

3.从hbase中的表作为数据源读取，hdfs作为数据输出

3.1 创建配置信息和作业对象，设置作业的类
Configuration conf = HBaseConfiguration.create();

Job job = new Job(conf, "job name ");
job.setJarByClass(test.class);
Scan scan = new Scan();
TableMapReduceUtil.initTableMapperJob(inputTable, scan, mapper.class, Writable.class, Writable.class, job);
job.setOutputKeyClass(Writable.class);
job.setOutputValueClass(Writable.class);

FileOutputFormat.setOutputPath(job, Path);
job.waitForCompletion(true);

3.2 mapper类：
public class mapper extends	TableMapper<KEYOUT, VALUEOUT>{ 
	public void map(Writable key, Writable value, Context context) throws IOException, InterruptedException {
			//mapper逻辑
			context.write(key, value);
		}
	}
}
 
3.3 reducer类：
public class reducer extends Reducer<Writable,Writable,Writable,Writable>{ 
	public void reducer(Writable key, Writable value, Context context)
			throws IOException, InterruptedException {
			//reducer逻辑
			context.write(key, value);
		}
	}
}


------------------------------------------------------------------------------------------------------------------------
4.最后说一下TableMapper和TableReducer的本质

其实这俩类就是为了简化一下书写代码，因为传入的4个泛型参数里都会有固定的参数类型，所以是Mapper和Reducer的简化版本，本质他们没有任何区别。

源码如下：
public abstract class TableMapper<KEYOUT, VALUEOUT> extends Mapper<ImmutableBytesWritable, Result, KEYOUT, VALUEOUT> {
}
 
public abstract class TableReducer<KEYIN, VALUEIN, KEYOUT> extends Reducer<KEYIN, VALUEIN, KEYOUT, Writable> {
}


